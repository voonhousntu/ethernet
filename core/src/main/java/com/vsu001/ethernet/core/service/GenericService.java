package com.vsu001.ethernet.core.service;

import com.google.cloud.bigquery.TableResult;
import com.google.protobuf.Descriptors.FieldDescriptor;
import java.io.FileNotFoundException;
import java.io.IOException;
import java.util.List;
import java.util.UUID;

public interface GenericService {

  String TABLE_NAME_PATTERN = "%s_%s_%s_%s";
  String ISO_STRING_PATTERN = "yyyyMMdd'T'HHmm'Z'";

  /**
   * Fetch the row(s) of interest from BigQuery.
   * <p>
   * The constraints of query, i.e. what to query for are defined in the UpdateRequest object.
   *
   * @param request The block-number range to query for is defined and wrapped in the UpdateRequest
   *                object. The user defined `start` and `end` object are inclusive when translated
   *                to the BigQuery legacy SQL query constraints equivalent.
   * @return The rows with block-number within the `start` and `end` number range (inclusive)
   * wrapped in a TableResult object.
   * @throws InterruptedException
   */
  TableResult fetchFromBq(UpdateRequest request) throws InterruptedException, FileNotFoundException;

  /**
   * Obtain the HDFS output path where the staging ORC file will be written to.
   *
   * @param nonce Suffix that is added to the tmp table that will only be used once for a single ETL
   *              session.
   * @return HDFS output path where the staging ORC file will be written to.
   */
  default String getOutputPath(String nonce) {
    return String.format("/user/hive/warehouse/%s/", nonce);
  }

  /**
   * Create a random (uuid) string for the staging ORC file that holds the incremental data before
   * it is ingested into the main Hive table.
   *
   * @return Random UUID string.
   */
  default String getFilename() {
    return UUID.randomUUID().toString() + ".orc";
  }

  /**
   * Obtain the actual Hive table name that will be used to persist a protobuf defined object of
   * interest.
   *
   * @return Hive table name.
   */
  String getTableName();

  /**
   * Obtain the temporary Hive table name that will be used to hold the incremental data before
   * being ingested into the main Hive table.
   *
   * @return Temporary Hive table name
   */
  String getTmpTableName();

  /**
   * Return the <code>FieldDescriptor</code> associated with the protobuf specification that the
   * current implementation of the <code>GenericService</code> is handling.
   *
   * @return FieldDescriptor of protobuf specification.
   */
  List<FieldDescriptor> getFieldDescriptors();

  /**
   * Obtain the Hive SQL schema string of Hive table which will be used to persist the protobuf
   * defined object of interest.
   *
   * @return Schema string of Hive SQL table which will be used to persist the protobuf defined
   * object of interest.
   */
  String getSchemaStr();

  /**
   * Obtain the struct string required to define the "schema" of an ORC file, which is used to
   * persist the protobuf defined object of interest.
   *
   * @return Struct string of ORC file that is used to persist the protobuf defined object of
   * interest.
   */
  String getStructStr();

  /**
   * Create Graph representation of tabular data from Hive backing store.
   * <p>
   * The database name will be automatically generated.
   *
   * @param request The block-number range to query for is defined and wrapped in the UpdateRequest
   *                object. The user defined `start` and `end` object are inclusive when translated
   *                to the BigQuery legacy SQL query constraints equivalent.
   * @param nonce   A random generated string that will only be used once to identify all assets
   *                associated with the an ETl.
   * @return The Neo4j database name of the graph created.
   * @throws IOException If an I/O error occurs
   */
  String doNeo4jImport(UpdateRequest request, String nonce) throws IOException;

  /**
   * Create Graph representation of tabular data from Hive backing store.
   *
   * @param databaseName Name of graph database.
   * @param request      The block-number range to query for is defined and wrapped in the
   *                     UpdateRequest object. The user defined `start` and `end` object are
   *                     inclusive when translated to the BigQuery legacy SQL query constraints
   *                     equivalent.
   * @param nonce        A random generated string that will only be used once to identify all
   *                     assets associated with the an ETl.
   * @return The Neo4j database name of the graph created.
   * @throws IOException If an I/O error occurs
   */
  String doNeo4jImport(String databaseName, UpdateRequest request, String nonce) throws IOException;

  /**
   * Create a Neo4j database name based on the block start and end ranges and date of creation.
   *
   * @param blockStartNo The first block number in the range.
   * @param blockEndNo   The last block number in the range.
   * @return The autogenerated database name with a structure of {network_type}_{block_start_no}_{block_end_no}_{datetime_of_etl}
   */
  String generateNeo4jDbName(long blockStartNo, long blockEndNo);

  /**
   * Update the cache file with the ranges that have been fetched from BigQuery.
   *
   * @param request The block-number range to query for is defined and wrapped in the UpdateRequest
   *                object. The user defined `start` and `end` object are inclusive when translated
   *                to the BigQuery legacy SQL query constraints equivalent.
   */
  void updateCache(UpdateRequest request) throws IOException;

}
